# Training a Model

- [Process Overview](#process-overview)
- [Setting Up](#setting-up)
- The Training Process:
    - [Extending a Grammar](#extending-a-grammar)
    - [Generating New Data](#generating-new-data)
    - [Pre-Processing Generated Data](#pre-processing-generated-data)
    - [Retraining a Model](#retraining-a-model)
    - [Managing Output Data](#managing-output-data)
- [Running With a New Model](#running-with-a-new-model)

## Process Overview

When the Arsenal NL to CST module runs, it takes inputs that are natural-language sentences and gives outputs that are concrete syntax trees, based on a model that is trained on a grammar over sentences expressing the grammar generated by "pretty-printing" functions.  
Whenever you modify either the grammar or its pretty-printing functions, you need to retrain the model.  Retraining gives a new model, which the Arsenal NL to CST module then uses at runtime.

## Setting Up 

This section covers special setup required for training a model.  

- [GPU Requirements](#gpu-requirements)
- [Special Software Requirements](#special-software-requirements)
- [Time for Training](#time-for-training)
- [Remote Machine Recommendation](#remote-machine-recommendation)

### GPU Requirements

Training a model requires a GPU.

GPU hardware and software <span style="text-decoration: underline;">requirements</span> include:
- a [NVIDIA GPU](https://developer.nvidia.com/cuda-gpus) 
- the [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) and the CUDA Deep Neural Network Library, [cuDNN](https://developer.nvidia.com/cudnn)
- a [NVIDIA Developer Program](https://developer.nvidia.com/developer-program) account to [download cuDNN](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#download) 


So far as <span style="text-decoration: underline;">installation</span>:
- CUDA and cuDNN can be installed in a local directory and added to `LD_LIBRARY_PATH`
- CUDA has an installer
- cuDNN has a downloadable archive that is unpacked for installation; its unpacked contents should be placed in correct locations in the CUDA installation - `cudnn.h` in the `include` directory, and `cudnn.so` in the `lib` directory

So far as <span style="text-decoration: underline;">versions</span> to install, the following versions - or any versions near these - should work:
- CUDA v. 10
- cuDNN v. 7.5

### Special Software Requirements

Training a model requires the following special software:

- [Pytorch](https://pytorch.org/get-started/locally/)
- [flask](https://pypi.org/project/Flask/)
- [matplotlib](https://matplotlib.org/users/installing.html)
- [tqdm](https://pypi.org/project/tqdm/)
- [tensorflow](https://www.tensorflow.org/install)

    For **Conda** as your package manager, first create an environment for Arsenal.  You have to do this only once.
    ```shell
    conda create -n arsenal python=3.7
    ```
    Next, activate the Conda environment for Arsenal.  Do this whenever you work with the NL to CST module.
    ```shell
    conda activate arsenal
    ```
    Then, install Pytorch in your activated Conda environment:
    ```shell
    conda install -c pytorch pytorch
    ```
    Then install the remaining dependencies explicitly using `conda install`:
    ```shell
    conda install -c conda-forge flask matplotlib tqdm tensorflow
    ```

### Time for Training

In preparing for training, it is important to consider the time it takes to train a model, as well as training option values that affect training run time. 

Generally, it takes 24 or more hours to train or to retrain a model.  Layer and node option settings directly impact training time.  In recent training runs training a model with 1 million lines, and running on one NVIDIA GeForce GTX1080 Ti (GP102) GPU with 11 GB memory, training run times were:

- 30 hours for 2 layers of 256 nodes each
- 60 hours for 4 layers of 256 nodes each

### Remote Machine Recommendation

Because training takes a long time to run, it is recommended to run on a remote machine using the [tmux](https://github.com/tmux/tmux) terminal multiplexer or an equivalent, so you can start the training process and then detach.

## Extending a Grammar

To extend an existing grammar or its pretty-printing functions, see the following tutorials:
- [Extending a Grammar](./Extending-a-Grammar.md)
- [Expressing Conditions](./Expressing-Conditions.md)

## Generating New Data 

You generate new data using the generate executable that was built from your grammar.  For the [regexp example](../regexp/README.md) this is `REgenerate.native` "generate" command in the `regexp/generate-reformulate` directory: 

```
$ cd ../regexp/generate-reformulate
$ ./REgenerate.native 1000000 -polish -types > aug7_1_eng-pn.txt
```

This usage of the "generate" command generates one line *per* example with an English input format (`eng`) and a Polish notation (`pn`) output format.  

Note that model trainer expects its input files to follow a naming convention similar to that used to name these output files using `eng-pn`; see [below](#pre-processing-generated-data) for more details.

Also note that the output file name uses a file naming convention with a prefix that captures date and run information.

## Pre-Processing Generated Data

To pre-process the generated data before training, you divide the data into a training set and a validation set, and optionally generate a similarly-divided training and validation set for debugging.  

Generally, training data should be approximately 90% of the data in the set, and validation data should be approximately 10%.  You may create a smaller training and validation set for debugging, e.g. using approximately 10% of the other data.

Naming conventions apply to the divided data files, which are expected for input to the model trainer.  These naming conventions indicate the source or input language and the target or output language, as follows:

```shell
$SRC-$TGT.train.txt
$SRC-$TGT.val.txt
```

For debugging data, the naming conventions are:

```shell
$SRC-$TGT.train.debug.txt
$SRC-$TGT.val.debug.txt
```

You may use Bash `head` and `tail` commands to divide the data.

To divide in to a training set (~90%) and a validation set (~10%) following the naming conventions:

```shell
$ head -n 999000 aug7_eng-pn.txt > eng-pn.train.txt
$ tail -n 1000 aug7_eng-pn.txt > eng-pn.val.txt
```

To optionally generate a debugging training set (9%) and debugging validation set (~1%):
```shell
$ head -n 100 aug7_eng-pn.txt > eng-pn.train.debug.txt
$ head -n 10 aug7_eng-pn.txt > eng-pn.val.debug.txt
```
Note that data set files should be moved into a data directory that is not code-controlled.

## Retraining a Model

You "retrain" a model after extending a grammar or its pretty-printing functions, or both, and both generating and pre-processing data.  An initial training and any subsequent retrainings involve the same command and options to "run the model trainer".

- [Running the Trainer](#running-the-trainer)
- [Input and Output Options](#input-and-output-options)
- [Initialization and Pattern-Matching Options](#initialization-and-pattern-matching-options)
- [Neural Network Options](#neural-network-options)
- [Other Options](#other-options)
- [Using the Output to Run](#using-the-output-to-run)

### Running the Trainer

You train the model or "run the model trainer" by executing the `train_seq2seq_single.py` Python script in the `seq2seq/src` directory, with option values for input & output data files and input & output languages, plus initialization & pattern-matching. For the regexp grammar this would be: 
```shell
$ cd ../seq2seq/src
$ python train_seq2seq_single.py \
—-data_root <directory with the generated divided data files> \
—-input_lang "eng" \
—-output_lang "pn" \
—-output_name <filename prefix> \
--init_type "re" \
--match_parens True 
```
### Input and Output Options

|Option|Default|Description|
|------|-------|-----------|
|`—-data_root <directory>`|-|Source directory for training data|
|`—-input_lang <lang>`|`eng`|String name for input language|
|`—-output_lang <lang>`|`cst`|String name for output language|
|`—-output_name <directory>`|-|Name for this experiment<br>(used as a directory prefix for file names)|

The `input_lang` and `output_lang` option values are not required, but the values `eng` for English input and `pn` for Polish notation output should be used for generated data. 

The `output_name` option value is the name of the output directory as a directory prefix for file names.  The directory will be created when files are saved.

The output directory will have two subdirectories to which files are saved: `logs` and `checkpoint`.
The `checkpoint` subdirectory will have files writter when the first checkpoint happens.
The `log` directory will have tensorboard-compatible log files.  

### Initialization and Pattern-Matching Options

|Option|Default|Description|
|------|-------|-----------|
|`--init_type <type>`|` "" `|Ensures correct typing of syntax trees by setting the grammar-specific initial type.|
|`—-match_parens <boolean>`|`False`|Match parentheses when parsing sentences.|

The value of the `init_type` option should always be set to the top type definition of the grammar to ensure that the trees are correctly typed.   The default value is the empty string  ` "" `, which should not be used.  For the regexp example this setting corresponds to the following definition of the `re` type in OCaml, in the REgrammar.ml](../regexp/generator-reformulate/src/REgrammar.ml) file:
```ocaml
type re = Terminal of terminal [@weight fun state -> 2. *. depth state ]
        | StartOfLine of re
        | EndOfLine of re
        | Plus of re
        | Star of re
        | Or of re*re
        | Concat of (re list[@random random_list ~min:2 random_re])
[@@deriving arsenal]
```
This setting can be generally understood as specifying the entry point to the grammar for any application. 

The value of the `match_parens` option should always be set to `True`.  Its default value is   `False`, which should not be used.

### Neural Network Options

|Option|Default|Description|
|------|-------|-----------|
|`—-num_layers <int>`|`2`|How many layers the neural network has. <br>You may try variations of this setting for the best validation accuracy.|
|`--num_hidden <int>`|`128`|How many "hidden" nodes *per* layer the neural network has.|

Recommended setting for layers and nodes are 2 to 4 layers of 256 nodes each.  Note that this recommendation involves always setting `num_hidden` to `256`, and optionally setting `num_layers` to `3` or `4`. 

### Other Options

The following option settings are optional, and their default values should work:

|Option|Default|Description|
|------|-------|-----------|
|`--iters <int>`|`1000000`|Number of
training iterations.  Generally, more iterations are better.  You may try variations of this setting to achieve the best validation accuracy.|
|`--debug_iters <int>`|`200`|Number
of training iterations, when debug is active.|
|`-—eval_every <int>`|`10000`|How frequently to evaluate against the validation results.|
|`—-sample_every <int>`|`100`|How frequently to sample to the tensorboard-compatible log.|
|`—-save_every <int>`|`10000`|How frequently to save model, vocabulary, and setup at a checkpoint.|

The following option settings are *not used* and *not recommended* for use:

|Option|Default|Description|
|------|-------|-----------|
|`max_length <int>`|`1000`|Maximum
number of tokens to consider in a sentence (input and output).|
|`-—reorder_numbered_placeholders <boolean>`|`"True"`|Reorder numbered placeholders (`_\d+$`) to orthographic order.|
|`—-dropout <float>`|`0.1`|Dropout (0 to 1)|
|`learning_rate`|`1e-3`||

### Using the Output to Run

The model runner or "runtime" will checkpoint.  You can use `ctrl-c` to stop the model runner, and then restart it from where it previously stopped, possibly with different parameters to improve validation accuracy. 

To view results, set the tensorboard to look at the log files, then look at the port that tensorboard port (by default 6000) *via* the web UI.  Set jupyter to port 5000 to view results. 

## Managing Output Data

Managing output data involves copying data files to an appropriate location for use in running the model.  For example:

```shell
$cp <outdir>/checkpoint/* <arsenal_home>/regexp/models/<new_model_dir>
```
where:
- `<outdir>/checkpoint` is the directory to which the files were output by training
- `<arsenal_home>` is the directory with the Arsenal modules
- `<new_model_dir>` is the model directory, for this edition of the model

The model directory will contain the following files: 

- `decoder`
- `encoder`
- `eng.vocab`
- `notes.txt`
- `pn.vocab`
- `setup.json`

The `setup.json` is used by the NL to CST module when running.

<!-- |File|Description|
|----|-----------|
|`decoder`||
|`encoder`||
|`eng.vocab`||
|`notes.txt`||
|`pn.vocab` ||
|`setup.json`|Used by the NL to CST module when running.| -->

### Running With a New Model

Once you have a new model, the NL to CST module needs to be able to find at runtime.
The NL to CST module finds the model location through the environment variable MODEL_ROOT. 
When running the NL to CST module directly through the Python executable `run_nl2cst_server.py` it is sufficient to set the MODEL_ROOT to point to the 
full path to `<new_model_dir>`.

```bash
$ export MODEL_ROOT=/path/to/new_model_dir
```

If running the NL to CST module via docker, update that docker-compose.yml file to mount the location of the directory that contains the new_model_dir
at "/opt/models" by using the "volumes:" key.  Set MODEL_ROOT in the docker-compose.yml file to put to the specific `<new_model_dir>` which is now 
available at the /opt/models mount point.

```yaml
  nl2cst:
    image: arsenal-docker.cse.sri.com/arsenal-nl2cst:latest
    volumes:
      - ./models:/opt/models:ro
    environment:
      NL2CST_HOST: nl2cst
      NL2CST_PORT: 8080
      MODEL_ROOT: /opt/models/my_new_model
    ports:
      - 8070:8080
```
